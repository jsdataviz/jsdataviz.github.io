{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfdcf185-3984-447c-836e-676257675ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import re\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0093c8d-a140-4cd2-8225-929885090832",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2024, 2023]\n",
    "registered_sexes = ['M', 'W']\n",
    "events = [\"I\", 'I60', 'I30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f5ae36e-56cb-44e9-8af7-7a118b0e1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rider_urls(soup):\n",
    "    # Find all parent elements with class 'type-fullname' and extract hrefs\n",
    "    hrefs = []\n",
    "    for element in soup.find_all(class_='type-fullname'):\n",
    "        # Find all anchor tags inside the parent element\n",
    "        a_tag = element.find('a')\n",
    "        if a_tag and a_tag.has_attr('href'):\n",
    "            hrefs.append(a_tag['href'])\n",
    "\n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816dc845-c969-4fcc-80b0-81156469c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ride_info(base_url, rider_url, event):\n",
    "    full_url = base_url + rider_url\n",
    "\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=5, backoff_factor=1, status_forcelist=[502, 503, 504])\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    \n",
    "    try:\n",
    "        response = session.get(full_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch data for {rider_url}, Status code: {response.status_code}\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame if the request fails\n",
    "\n",
    "        # Parse the HTML content from the GET response\n",
    "        get_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Initialize a list to store the pivoted tables\n",
    "        pivoted_tables = []\n",
    "        \n",
    "        # Define the classes containing tables\n",
    "        target_classes = ['box-general', 'box-totals', 'box-state']\n",
    "        for target_class in target_classes:\n",
    "            box = get_soup.find(class_=target_class)\n",
    "            if box:\n",
    "                html_string = str(box)\n",
    "                tables = pd.read_html(StringIO(html_string))\n",
    "                for table in tables:\n",
    "                    # Ensure the table has valid rows\n",
    "                    if not table.empty:\n",
    "                        # Pivoting the DataFrame\n",
    "                        pivoted_df = table.set_index(0).T.reset_index(drop=True)\n",
    "                        pivoted_df.columns.name = None  # Remove column names\n",
    "                        pivoted_tables.append(pivoted_df)\n",
    "        \n",
    "        # Handling the \"splits\" table\n",
    "        split_html = get_soup.find(class_='box-splits')\n",
    "        if split_html:\n",
    "            html_string = str(split_html)\n",
    "            tables = pd.read_html(StringIO(html_string))\n",
    "            if tables:\n",
    "                table = tables[0]\n",
    "\n",
    "                # Create a flattened dictionary for the new row format\n",
    "                flattened_data = {}\n",
    "\n",
    "                # Custom labels for columns\n",
    "                labels = {\n",
    "                    \"I\": ['arr25', 'dep26', 'arr53', 'dep54', 'arr73', 'dep74', 'finish'],\n",
    "                    \"I60\": ['arr25', 'dep26', 'arr32', 'dep33', 'finish'],\n",
    "                    \"I30\": ['finish']\n",
    "                }\n",
    "\n",
    "                # Extract the data based on the column labels\n",
    "                for i, label in enumerate(labels.get(event, [])):\n",
    "                    flattened_data[f'{label}_tod'] = table['Time Of Day'][i] if 'Time Of Day' in table.columns else 'N/A'\n",
    "                    flattened_data[f'{label}_time'] = table['Time'][i] if 'Time' in table.columns else 'N/A'\n",
    "                    flattened_data[f'{label}_diff'] = table['diff.'][i] if 'diff.' in table.columns else 'N/A'\n",
    "                    flattened_data[f'{label}_mph'] = table['mph'][i] if 'mph' in table.columns else 'N/A'\n",
    "\n",
    "                # Convert the flattened data dictionary back into a DataFrame with one row\n",
    "                split_df = pd.DataFrame([flattened_data])\n",
    "                pivoted_tables.append(split_df)\n",
    "\n",
    "        # Concatenate all the tables if there are any\n",
    "        if pivoted_tables:\n",
    "            concat_frame = pd.concat(pivoted_tables, axis=1, ignore_index=True)\n",
    "            return concat_frame\n",
    "        else:\n",
    "            print(f\"No data found for {rider_url}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if no tables were found\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for {full_url}: {e}. Retrying now\")\n",
    "        i -= 1\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if the request fails\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError for {rider_url}: {e}\")\n",
    "        return pd.DataFrame()  # Skip this rider and return an empty DataFrame\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {rider_url}: {e}\")\n",
    "        return pd.DataFrame()  # Skip this rider on unexpected errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "968656a0-0a1e-4ddf-b1c2-7d75d2f00ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pages(year, event, sex):\n",
    "\n",
    "    base_url = f'https://results.ridelondon.co.uk/{year}/'\n",
    "\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=5, backoff_factor=1, status_forcelist=[502, 503, 504])\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    \n",
    "    query_params = {\n",
    "        \"page\": \"1\",\n",
    "        \"event\": event,\n",
    "        \"num_results\": \"100\",\n",
    "        \"pid\": \"list\",\n",
    "        \"pidp\": \"start\",\n",
    "        \"search[sex]\": sex,\n",
    "    }\n",
    "\n",
    "    response = session.get(base_url, params=query_params, timeout=10)\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    match = re.search(r'(\\d+) Times', response.text)\n",
    "    number_of_times = int(match.group(1))\n",
    "    number_of_pages = math.ceil(number_of_times / 100)\n",
    "    step = number_of_pages // 10\n",
    "    \n",
    "    # Final DataFrame to hold the concatenated data from all pages\n",
    "    \n",
    "    for i in range(1, number_of_pages + 1):\n",
    "        try:\n",
    "            query_params[\"page\"] = i\n",
    "            response = session.get(base_url, params=query_params, timeout=10)\n",
    "            response.raise_for_status()  # Check for errors\n",
    "\n",
    "            # Parse and process page data here\n",
    "            rider_frames = []\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "\n",
    "            hrefs = get_rider_urls(soup)\n",
    "            for href in hrefs:\n",
    "                rider_frame = get_ride_info(base_url, href, query_params[\"event\"])\n",
    "                rider_frames.append(rider_frame)\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed on page {i}: {e}. Retrying now.\")\n",
    "            i -= 1\n",
    "            continue  # Skip to the next page on failure\n",
    "    \n",
    "        final_frame = pd.concat(rider_frames)\n",
    "    \n",
    "        final_frame['sex'] = query_params['search[sex]']\n",
    "        final_frame['year'] = year\n",
    "    \n",
    "        final_frame = final_frame.rename(columns={\n",
    "            'Name': 'name',\n",
    "            'Rider number': 'rider_number',\n",
    "            'Charity': 'charity',\n",
    "            'Event': 'event',\n",
    "            'Finish': 'finish_time',\n",
    "            'Status': 'status',\n",
    "            'Last Timing Point': 'last_timing_point'\n",
    "        })\n",
    "        \n",
    "        final_frame.to_csv(f\"data/{year}_event_{query_params[\"event\"]}_{query_params[\"search[sex]\"]}_page_{query_params[\"page\"]}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3838a00-46fa-485b-b15e-20b046f9b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    for event in events:\n",
    "        for sex in registered_sexes:\n",
    "            get_all_pages(year, event, sex)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Loop over all files in the directory\n",
    "for filename in os.listdir(\"/data\"):\n",
    "    if filename.endswith('.csv'):  # Check if the file is a CSV\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Read the CSV file and append the DataFrame to the list\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combine all dataframes into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "combined_df.to_csv(\"final_ride_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71a33f-b900-4668-9913-59037f6951b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
